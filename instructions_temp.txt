
üöÄ Midterm Coding Assignment
Insurance Claim Timeline Retrieval System (GenAI + Agents + MCP)
Published: 8/12 Due: Thursday 18/12

üìå Overview
In this coding assignment, you will implement a small-scale GenAI system that manages and queries an insurance claim timeline using:
	‚Ä¢	Multi-agent orchestration
	‚Ä¢	Data segmentation & indexing
	‚Ä¢	Retrieval methods (summary index + hierarchical chunk index)
	‚Ä¢	MCP tool integration
	‚Ä¢	LLM-as-a-judge evaluation
This is a coding exercise ‚Äî you are expected to implement all components in Python using llamaindex/langchain
A detailed explanation of design pagechoices must be included in a README.md file.




üß© Assignment Requirements

1. Implement a SINGLE Insurance Claim File (Timeline-Based)
You will either:
	‚Ä¢	Generate synthetic insurance claim documents.
Your data file pdf must include:
	‚Ä¢	Events over a timeline - time-series chronological events
	‚Ä¢	Data granularity - second/minutes resolution
	‚Ä¢	At least 10 pages of content
	‚Ä¢	A case with sparse or hard-to-find details ("needle" data)

Section 2 ‚Äî Data Management & Indexing (Condensed + Bullets)
Effective retrieval begins with representing the claim data in a hierarchical structure (e.g., Claim ‚Üí Document ‚Üí Section ‚Üí Chunk). This structure supports selective retrieval, metadata filtering, and alignment between query types and information granularity.
	‚Ä¢	Enables structured navigation across documents
	‚Ä¢	Preserves relationships between parts of the claim
	‚Ä¢	Supports different retrieval depths depending on the agent‚Äôs needs
To handle both broad and highly specific queries, each document must be chunked into multiple levels of granularity (small / medium / large chunks). This design enables the Auto-Merging Retriever approach, where the system starts from fine-grained chunks and automatically merges upward when additional context is required.
	‚Ä¢	Small chunks ‚Üí high-precision factual retrieval
	‚Ä¢	Medium chunks ‚Üí balanced reasoning
	‚Ä¢	Large chunks ‚Üí broader context reconstruction
Additionally, create a Summary Index to answer high-level or timeline-oriented questions efficiently. Build it using a MapReduce summarization strategy, where each chunk is summarized first (‚ÄúMap‚Äù), then combined into section and document-level summaries (‚ÄúReduce‚Äù).
	‚Ä¢	Supports fast summary queries without long-context retrieval
	‚Ä¢	Stores timeline overviews, key entities, and section summaries
	‚Ä¢	Serves as the knowledge base for summary-focused agents

The README must explain:
	‚Ä¢	Chunk size strategy
	‚Ä¢	Overlap strategy
	‚Ä¢	Why you chose the hierarchy depth
	‚Ä¢	How recall is improved by your segmentation choices

3. Implement Three Agents (Coding Required)
You must implement the following agents in code:
1. Manager (Router) Agent
	‚Ä¢	Receives user query
	‚Ä¢	Determines the correct agent to call
	‚Ä¢	Chooses which index to use (summary vs hierarchical)
2. Summarization Expert Agent
	‚Ä¢	Answers high-level questions
	‚Ä¢	Uses the Summary Information Index
3. Needle-in-a-Haystack Agent
	‚Ä¢	Handles precise factual queries
	‚Ä¢	Searches deep inside the hierarchical index
Your code should demonstrate:
	‚Ä¢	Routing logic
	‚Ä¢	Use of indexes
	‚Ä¢	Use of model prompts as functions

4. MCP Integration (Coding Required)
Implement an MCP tool call in one of these ways:
	‚Ä¢	Access claim documents
	‚Ä¢	Retrieve metadata via a tool
	‚Ä¢	Perform a computation (e.g., date diff, cost estimation)
	‚Ä¢	Validate document status
The goal: Show that the LLM uses MCP to extend its capabilities beyond prompting.

5. Agent Diagram (Submit as a File)
Create a diagram (draw.io, Figma, or even Paint) showing:
	‚Ä¢	Manager ‚Üí Sub-agent routing
	‚Ä¢	Flow of data between indexes and agents
	‚Ä¢	Point of MCP integration
Export as PNG/JPEG and submit.

6. System Evaluation (Coding Required)
Use a separate model to evaluate your system (‚ÄúLLM-as-a-judge‚Äù).
Evaluate based on:
A. Answer Correctness
Does answer match ground truth?
B. Context Relevancy
Did agent use the correct index and relevant segments?
C. Context Recall
Did the system retrieve the correct chunk(s)?
You must implement:
	‚Ä¢	Judge prompts
	‚Ä¢	Evaluation functions
	‚Ä¢	A small test suite (at least 6‚Äì8 queries)
Summaries go in the main document. Full evaluation goes in README.

7. README.md (Mandatory)
Your README must include:
	‚Ä¢	Architecture explanation
	‚Ä¢	Data segmentation decisions
	‚Ä¢	Chunking rationale
	‚Ä¢	Index schemas
	‚Ä¢	Agent design + prompt structure
	‚Ä¢	MCP usage explanation
	‚Ä¢	Evaluation methodology + examples
	‚Ä¢	Limitations & trade-offs

8. Final Submission (1 Page + Code + Diagram)
You must submit:
	‚Ä¢	main.py (or equivalent orchestrator)
	‚Ä¢	Agent implementations
	‚Ä¢	Index implementation files
	‚Ä¢	MCP integration code
	‚Ä¢	Evaluation code
	‚Ä¢	Diagram (PNG/JPEG)
	‚Ä¢	README.md
Main PDF (1 page maximum):
	‚Ä¢	Short system overview
	‚Ä¢	Diagram + architecture
	‚Ä¢	Main results
	‚Ä¢	Brief explanation of MCP usage

üîç Professional Notes (Instructor Commentary)
This assignment reflects real-world GenAI engineering, not toy examples. Key skills tested:
‚úî Data segmentation for meaningful retrieval
The core of retrieval quality lies in how data is chunked and indexed. This assignment forces students to think like production RAG engineers.
‚úî Agentic reasoning with routing
Separating summarization vs precision queries teaches:
	‚Ä¢	task classification
	‚Ä¢	retrieval-aware routing
	‚Ä¢	specialized reasoning modules
‚úî MCP integration
This introduces tool-augmented models ‚Äî foundational for AI orchestration.
‚úî Evaluation with a separate judge model
This mirrors professional evaluation pipelines used in:
	‚Ä¢	insurance automation
	‚Ä¢	healthcare RAG systems
	‚Ä¢	legal document retrieval
	‚Ä¢	banking compliance workflows

If you'd like, I can now:
‚úÖ Generate a polished official PDF of the coding assignment
‚úÖ Provide a starter template (main.py, directories, class stubs)
‚úÖ Provide example test cases
‚úÖ Add rubric for grading

